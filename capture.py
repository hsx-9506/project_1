import cv2, json
import mediapipe as mp
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
from tensorflow.keras.models import load_model
import numpy as np

# è®€å–è¨­å®š
with open(".vscode/setting.json", 'r', encoding='utf8') as jfile:
    jdata = json.load(jfile)

# è®€å–æ‰‹å‹¢è­˜åˆ¥æ¨¡å‹
model = load_model("gesture_model.h5")
gesture_labels = ["victory âœŒï¸", "fist âœŠ", "ok ğŸ‘Œ", "middle ğŸ–•", "thumbs_up ğŸ‘", "heart ğŸ¤"]

# é è¨­ä½¿ç”¨ USB æ”å½±æ©Ÿ (0)ï¼Œè‹¥ç„¡æ³•é–‹å•Ÿå‰‡æ”¹ç”¨ä¸²æµ
video_sources = [0, jdata["video_source"]]
current_camera = 0

# é–‹å•Ÿæ”å½±æ©Ÿ
def open_video_source(index):
    cap = cv2.VideoCapture(video_sources[index])
    if cap.isOpened():
        print(f"å·²é–‹å•Ÿæ”å½±æ©Ÿ: {video_sources[index]}\n")
        return cap
    else:
        cap.release()
        raise ValueError(f"ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ: {video_sources[index]}")

class App:
    def __init__(self, window):
        self.window = window
        self.window.title("ğŸ– æ‰‹å‹¢æ•¸å­— & AI æ‰‹å‹¢è­˜åˆ¥")
        self.window.configure(bg="#1e1e1e")

        self.model = load_model("gesture_model.h5")
        with open(".vscode/gesture_labels.json", "r", encoding='utf8') as f:
            self.gesture_labels = json.load(f)

        global current_camera
        self.cap = open_video_source(current_camera)

        # è¨­å®š UI é¢¨æ ¼
        style = ttk.Style()
        style.theme_use("clam")
        style.configure("TFrame", background="#252526")  
        style.configure("TLabel", background="#252526", foreground="white", font=("Microsoft JhengHei", 14, "bold"))
        style.configure("Rounded.TButton", background="#3e3e3e", foreground="white", 
                        font=("Microsoft JhengHei", 12, "bold"), padding=10, borderwidth=0, relief="flat")
        style.map("Rounded.TButton", background=[("active", "#505050")])  

        # ä¸»è¦ UI ä½ˆå±€
        self.video_frame = tk.Frame(window, bg="#1e1e1e")
        self.control_frame = tk.Frame(window, bg="#252526", width=200)

        self.video_frame.grid(row=1, column=0, padx=10, pady=10, sticky="nsew")
        self.control_frame.grid(row=1, column=1, padx=10, pady=10, sticky="ns")

        window.grid_rowconfigure(1, weight=1)
        window.grid_columnconfigure(0, weight=3)  
        window.grid_columnconfigure(1, weight=1)  

        # å½±åƒé¡¯ç¤ºå€
        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        self.canvas = tk.Canvas(self.video_frame, width=self.width, height=self.height, bg="#1e1e1e",
                                highlightthickness=0, borderwidth=0)  
        self.canvas.pack(fill="both", expand=True)  

        # å·¦æ‰‹é¡¯ç¤ºå€
        self.left_hand_label = tk.Label(self.control_frame, text="å·¦æ‰‹æ¯”çš„æ•¸å­—ï¼š", font=("Microsoft JhengHei", 14, "bold"), 
                                        bg="#252526", fg="white")
        self.left_hand_label.pack(pady=5)
        self.left_hand_text = tk.StringVar()
        self.left_hand_display = tk.Label(self.control_frame, textvariable=self.left_hand_text, font=("Microsoft JhengHei", 12), 
                                          bg="#252526", fg="white")
        self.left_hand_display.pack(pady=5)

        # å³æ‰‹é¡¯ç¤ºå€
        self.right_hand_label = tk.Label(self.control_frame, text="å³æ‰‹æ¯”çš„æ•¸å­—ï¼š", font=("Microsoft JhengHei", 14, "bold"), 
                                         bg="#252526", fg="white")
        self.right_hand_label.pack(pady=5)
        self.right_hand_text = tk.StringVar()
        self.right_hand_display = tk.Label(self.control_frame, textvariable=self.right_hand_text, font=("Microsoft JhengHei", 12), 
                                           bg="#252526", fg="white")
        self.right_hand_display.pack(pady=5)

        # æŒ‰éˆ•å€
        self.switch_button = ttk.Button(self.control_frame, text="åˆ‡æ›æ”å½±æ©Ÿ (C)", style="Rounded.TButton", command=self.switch_camera)
        self.switch_button.pack(pady=10)

        self.mode_label = tk.Label(self.control_frame, text="ç›®å‰æ¨¡å¼ï¼šæ•¸å­—è¾¨è­˜æ¨¡å¼", font=("Microsoft JhengHei", 12),
                                   bg="#252526", fg="lightgreen")
        self.mode_label.pack(pady=10)
        self.mode_button = ttk.Button(self.control_frame, text="åˆ‡æ›æ¨¡å¼ (M)", 
                                      style="Rounded.TButton", command=self.switch_mode)
        self.mode_button.pack(pady=5)

        self.exit_button = ttk.Button(self.control_frame, text="é€€å‡º", style="Rounded.TButton", command=self.on_closing)
        self.exit_button.pack(pady=10)

        # è®Šæ•¸
        self.is_advanced_mode = False
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(static_image_mode=False, max_num_hands=2,
                                         min_detection_confidence=0.7, min_tracking_confidence=0.5)
        self.mp_draw = mp.solutions.drawing_utils

        self.delay = 5
        self.update()
        self.window.protocol("WM_DELETE_WINDOW", self.on_closing)

        self.window.bind("<c>", lambda event: self.switch_camera())
        self.window.bind("<m>", lambda event: self.switch_mode())

    def detect_number(self, hand_landmarks, is_right):
        """è¨ˆç®—æ‰‹æ¯”çš„æ•¸å­— (åŒ…å«æ‹‡æŒ‡)"""
        count = 0
        finger_tips = [8, 12, 16, 20]  # é£ŸæŒ‡ï½å°æŒ‡
        finger_pips = [6, 10, 14, 18]
        for tip, pip in zip(finger_tips, finger_pips):
            if hand_landmarks.landmark[tip].y < hand_landmarks.landmark[pip].y:
                count += 1

        # **æ‹‡æŒ‡åˆ¤å®š**
        thumb_tip = hand_landmarks.landmark[4]
        thumb_ip = hand_landmarks.landmark[3]

        if is_right and thumb_tip.x < thumb_ip.x:
            count += 1
        elif not is_right and thumb_tip.x > thumb_ip.x:
            count += 1

        return count
    
    def predict_gesture(self, hand_landmarks):
        """ä½¿ç”¨ AI æ¨¡å‹é æ¸¬æ‰‹å‹¢"""
        landmarks = []
        for lm in hand_landmarks.landmark:
            landmarks.extend([lm.x, lm.y])

        # è½‰æ›ç‚º NumPy é™£åˆ—ä¸¦é€²è¡Œé æ¸¬
        data = np.array(landmarks).reshape(1, 42)
        prediction = self.model.predict(data)[0]
        
        # å–å¾—æ©Ÿç‡æœ€é«˜çš„æ‰‹å‹¢
        top_index = np.argmax(prediction)
        confidence = prediction[top_index]

        if confidence >= 0.7:
            return f"{self.gesture_labels[top_index]} ({confidence*100:.1f}%)"
        else:
            return "ä¸ç¢ºå®š ğŸ¤”"

    def update(self):
        ret, frame = self.cap.read()
        if ret:
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.hands.process(image)

            left_result, right_result = "æœªåµæ¸¬", "æœªåµæ¸¬"

            if results.multi_hand_landmarks and results.multi_handedness:
                for handLms, handLabel in zip(results.multi_hand_landmarks, results.multi_handedness):
                    label = handLabel.classification[0].label  # Left or Right
                    
                    # æ ¹æ“šæ¨¡å¼é¸æ“‡æ•¸å­—è­˜åˆ¥æˆ– AI æ‰‹å‹¢è­˜åˆ¥
                    if self.is_advanced_mode:
                        result = self.predict_gesture(handLms)  # ä½¿ç”¨ AI æ‰‹å‹¢è­˜åˆ¥
                    else:
                        result = str(self.detect_number(handLms, label == "Right"))  # ä½¿ç”¨æ•¸å­—è¾¨è­˜

                    # æ ¹æ“šæ‰‹çš„é¡å‹ï¼ˆå·¦ / å³ï¼‰ä¾†æ›´æ–°çµæœ
                    if label == "Left":
                        left_result = result
                    elif label == "Right":
                        right_result = result

                    self.mp_draw.draw_landmarks(image, handLms, self.mp_hands.HAND_CONNECTIONS)

            self.left_hand_text.set(left_result)
            self.right_hand_text.set(right_result)

            self.photo = ImageTk.PhotoImage(image=Image.fromarray(image))
            self.canvas.create_image(0, 0, image=self.photo, anchor=tk.NW)

        self.window.after(self.delay, self.update)

    def switch_camera(self):
        global current_camera
        current_camera = (current_camera + 1) % len(video_sources)
        self.cap.release()
        self.cap = open_video_source(current_camera)

    def switch_mode(self):
        """åˆ‡æ›æ¨¡å¼ï¼Œä¸¦ä¿®æ”¹ UI æ–‡å­—"""
        self.is_advanced_mode = not self.is_advanced_mode
        mode_text = "æ‰‹å‹¢è­˜åˆ¥æ¨¡å¼" if self.is_advanced_mode else "æ•¸å­—è¾¨è­˜æ¨¡å¼"
        self.mode_label.config(text=f"ç›®å‰æ¨¡å¼ï¼š{mode_text}")

        # ä¿®æ”¹ UI æ–‡å­—
        if self.is_advanced_mode:
            self.left_hand_label.config(text="å·¦æ‰‹çš„æ‰‹å‹¢ï¼š")
            self.right_hand_label.config(text="å³æ‰‹çš„æ‰‹å‹¢ï¼š")
        else:
            self.left_hand_label.config(text="å·¦æ‰‹æ¯”çš„æ•¸å­—ï¼š")
            self.right_hand_label.config(text="å³æ‰‹æ¯”çš„æ•¸å­—ï¼š") 

    def on_closing(self):
        if self.cap.isOpened():
            self.cap.release()
        self.window.destroy()

if __name__ == '__main__':
    root = tk.Tk()
    App(root)
    root.mainloop()
